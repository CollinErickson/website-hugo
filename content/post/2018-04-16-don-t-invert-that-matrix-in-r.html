---
title: Don't invert that matrix in R
author: Collin Erickson
date: '2018-04-16'
slug: don-t-invert-that-matrix-in-r
categories: []
tags: []
---



<p>There’s a somewhat famous blog post titled <a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">Don’t Invert That Matrix</a>. This topic has also been discussed using R <a href="http://civilstat.com/2015/07/dont-invert-that-matrix-why-and-how/">on this blog post</a>. You should read those instead of this. I’m just experimenting to see how important this concept is in R, especially in the cases I use often.</p>
<div id="the-main-idea" class="section level2">
<h2>The main idea</h2>
<p>Often finding the inverse of a matrix is not the endgoal. If it is, then you have do invert it. Otherwise you are usually trying to solve a system of equations, <span class="math inline">\(Ax=b\)</span>, for the vector <span class="math inline">\(x\)</span>, where <span class="math inline">\(A\)</span> is a matrix and <span class="math inline">\(b\)</span> is a given vector. You may need to solve this for many different <span class="math inline">\(b\)</span> or for a single one. If you only need to solve it for a single <span class="math inline">\(b\)</span>, you can simply run <code>solve(A, b)</code> and you are done.</p>
<p>It becomes more of an issue when you need to solve the equation for many different <span class="math inline">\(b\)</span>. Using <code>solve</code> is an <span class="math inline">\(O(n^3)\)</span> operation, where <span class="math inline">\(n\)</span> is the number of rows of <span class="math inline">\(A\)</span>, meaning it is slow. In small dimensions, you can just invert <span class="math inline">\(A\)</span> by storing the result of <code>solve(A)</code>, and multiplying this by each <span class="math inline">\(b\)</span>. If you have all your <span class="math inline">\(b\)</span>’s at once, say in matrix <span class="math inline">\(B\)</span>, you can use <code>solve(A, B)</code> and you are done. So now the problem is really that you need to solve the equation for many different <span class="math inline">\(b\)</span>, and you don’t have all the <span class="math inline">\(b\)</span>’s in the beginning, and that <span class="math inline">\(A\)</span> is big. Think of having to solve an iterative equation.</p>
<p>Having to do an <span class="math inline">\(O(n^3)\)</span> operation repeatedly is slow and a waste of resources. You could invert <span class="math inline">\(A\)</span> once, then multiply that inverse times each <span class="math inline">\(b\)</span> when needed. The problem with this is that calculating the inverse is not as numerically stable as factorizing the matrix.</p>
</div>
<div id="factoring-a-matrix" class="section level2">
<h2>Factoring a matrix</h2>
<p>Factoring a matrix means you find matrices whose product is equal to <span class="math inline">\(A\)</span>. An LU-decomposition (<code>Matrix::lu</code>) finds a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span> such that <span class="math inline">\(LU=A\)</span>. If <span class="math inline">\(A\)</span> is not square, then it can be factored as <span class="math inline">\(A=PLU\)</span>. If <span class="math inline">\(A\)</span> as symmetric and positive definite, then the Cholesky factorization (<code>chol</code>) gives an upper triangular matrix <span class="math inline">\(R\)</span> such that <span class="math inline">\(R^T R = A\)</span>. Other matrix decompositions include QR factorization and SVD.</p>
<p>Once you have a factorization of <span class="math inline">\(A\)</span>, you can use the factors for each <span class="math inline">\(b\)</span> to solve the system. Usually the factors are triangular or diagonal. The benefit of doing this is that it is much more numerically stable to solve using the factors then with the full inverse of <span class="math inline">\(A\)</span>.</p>
</div>
<div id="testing-with-correlation-matrices" class="section level2">
<h2>Testing with correlation matrices</h2>
<p>I’m going to use correlation matrices to test this, since that is what I use for Gaussian process models. These matrices are positive definite, so I can use the Cholesky decomposition. Below I create a 100x100 correlation matrix and plot it.</p>
<pre class="r"><code>set.seed(0)
d &lt;- 4
n &lt;- 100
theta &lt;- 2 #c(.05,.1,.05,1)
X &lt;- matrix(runif(d*n), ncol=d)
A &lt;- outer(1:n, 1:n,
           Vectorize(
             function(i,j) {
               exp(-sum(theta*(X[i,]-X[j,])^2))
             }
           ))
image(A)</code></pre>
<p><img src="/post/2018-04-16-don-t-invert-that-matrix-in-r_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>To see how poorly conditioned it is, we can check the eigenvalues.</p>
<pre class="r"><code>summary(eigen(A, only.values = T)$values)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##  0.00003  0.00169  0.01744  1.00000  0.23363 38.44173</code></pre>
<p>The smallest eigenvalue is about 0.00003, which is not great. The determinant of the <span class="math inline">\(A\)</span> is 2.07338210^{-170}, which makes me think it is poorly conditioned.</p>
<p>To see how the accuracy of the solve is, I’ll create a random vector <span class="math inline">\(x\)</span> and calculate <span class="math inline">\(b\)</span>.</p>
<pre class="r"><code>set.seed(1)
x &lt;- rnorm(100)
b &lt;- A %*% x</code></pre>
<p>First we’ll check the difference between x and using <code>solve</code>.</p>
<pre class="r"><code>summary(solve(A, b) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-3.262e-11  
##  1st Qu.:-1.552e-12  
##  Median :-8.965e-14  
##  Mean   :-4.600e-16  
##  3rd Qu.: 1.378e-12  
##  Max.   : 2.962e-11</code></pre>
<p>Largest difference is about 3e-11.</p>
<p>Now we can see how far off the solution is when using the inverse compared to the correct answer.</p>
<pre class="r"><code>A_inv &lt;- solve(A)
summary(A_inv %*% b - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-1.048e-10  
##  1st Qu.:-6.826e-12  
##  Median :-7.054e-13  
##  Mean   :-1.248e-13  
##  3rd Qu.: 8.203e-12  
##  Max.   : 1.142e-10</code></pre>
<p>The largest error is about 1.1e-10, which is 3x larger than when using <code>solve</code>.</p>
<p>Now using <code>backsolve</code> twice with the Cholesky factorization:</p>
<pre class="r"><code>A_chol &lt;- chol(A)
summary(backsolve(A_chol, backsolve(A_chol, b, transpose = T)) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-2.092e-11  
##  1st Qu.:-1.043e-12  
##  Median : 5.845e-14  
##  Mean   :-2.980e-16  
##  3rd Qu.: 1.452e-12  
##  Max.   : 1.790e-11</code></pre>
<p>Differences are as large as 2e-11, which is even smaller than when using <code>solve</code>.</p>
<p>How about LU decomposition? I tried first using <code>Matrix::lu</code>, but had issues. Instead I used <code>matrixcalc::lu.decomposition</code>.</p>
<pre class="r"><code>library(matrixcalc)
A_lu &lt;- lu.decomposition(A)
summary(c(A_lu$L %*% A_lu$U - A))</code></pre>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -7.772e-16 -5.551e-17  0.000e+00  3.841e-19  5.551e-17  7.772e-16</code></pre>
<pre class="r"><code># Check that decomposition is correct, all &lt; 1e-15
summary(backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri=F)) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-3.248e-11  
##  1st Qu.:-1.627e-12  
##  Median : 2.096e-14  
##  Mean   :-3.500e-16  
##  3rd Qu.: 1.319e-12  
##  Max.   : 2.727e-11</code></pre>
<p>The biggest errors are about 3e-11, which is about the same as using <code>solve</code>, better than using the inverse, and worse than using Cholesky.</p>
<p>So far it seems that doing the full inverse is the least accurate, while the decompositions are about as accurate as using <code>solve</code>.</p>
</div>
<div id="using-hilbert-matrices" class="section level2">
<h2>Using Hilbert matrices</h2>
<p>The symmetric Hilbert matrices are known for being numerically unstable. I tried to invert the 20x20 Hilbert matrix, but R gave an error. The largest that is invertible is the 11x11.</p>
<pre class="r"><code>set.seed(2)
n &lt;- 11
A &lt;- matrix(Matrix::Hilbert(n), n, n)
x &lt;- rnorm(n)
b &lt;- A %*% x</code></pre>
<p>Using <code>solve</code>, the max error is about 6e-3.</p>
<pre class="r"><code>summary(solve(A, b) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-4.988e-03  
##  1st Qu.:-5.101e-04  
##  Median :-3.000e-09  
##  Mean   : 0.000e+00  
##  3rd Qu.: 1.040e-03  
##  Max.   : 5.973e-03</code></pre>
<p>Finding the inverse, the max error is about 2e-2, so about three times larger.</p>
<pre class="r"><code>A_inv &lt;- solve(A)
summary(A_inv %*% b - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-0.0210056  
##  1st Qu.:-0.0058718  
##  Median :-0.0004449  
##  Mean   :-0.0004714  
##  3rd Qu.: 0.0035936  
##  Max.   : 0.0171158</code></pre>
<p>Using Cholesky, it is half as much as <code>solve</code>, 1e-2.</p>
<pre class="r"><code>A_chol &lt;- chol(A)
summary(backsolve(A_chol, backsolve(A_chol, b, transpose = T)) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-8.487e-03  
##  1st Qu.:-8.691e-04  
##  Median :-5.000e-09  
##  Mean   : 0.000e+00  
##  3rd Qu.: 1.773e-03  
##  Max.   : 1.017e-02</code></pre>
<p>And with LU decomposition the max error is half as large as that, 4e-3, or a little better than <code>solve</code>.</p>
<pre class="r"><code>A_lu &lt;- lu.decomposition(A)
summary(c(A_lu$L %*% A_lu$U - A))</code></pre>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -2.776e-17  0.000e+00  0.000e+00  1.147e-19  0.000e+00  2.776e-17</code></pre>
<pre class="r"><code># Check that decomposition is correct, all &lt; 1e-16
summary(backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri=F)) - x)</code></pre>
<pre><code>##        V1            
##  Min.   :-3.328e-03  
##  1st Qu.:-3.426e-04  
##  Median :-2.000e-09  
##  Mean   : 0.000e+00  
##  3rd Qu.: 7.016e-04  
##  Max.   : 3.998e-03</code></pre>
<p>We are getting much larger errors than with the correlation matrix, up to about 1e-2 instead of 1e-11. This means that the conditioning of the matrix has a large effect on how accurate these solves are.</p>
<p>Again we see that finding the inverse is the least stable calculation, but not by a huge margin.</p>
</div>
<div id="downside-of-using-decomposition" class="section level2">
<h2>Downside of using decomposition</h2>
<p>One downside of using the decomposition instead of the inverse is that it is a bit harder to use in calculations. Instead of <code>A_inv %*% b</code>, you have to do <code>backsolve(A_chol, backsolve(A_chol, b, transpose = T))</code>. You would probably want to write a function for it, or just create a new class.</p>
<p>A second downside is that it is slower. So if you only have to factor a matrix once, but solve the system of equations for hundreds of <code>b</code>’s, it may take way longer. Below is a benchmark comparing the times of using the inverse vs the Cholesky factorization vs the LU factorization.</p>
<pre class="r"><code>microbenchmark::microbenchmark(
  A_inv %*% b,
  backsolve(A_chol, backsolve(A_chol, b, transpose = T)),
  backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri=F))
)</code></pre>
<pre><code>## Unit: nanoseconds
##                                                    expr  min    lq
##                                             A_inv %*% b  380   380
##  backsolve(A_chol, backsolve(A_chol, b, transpose = T)) 9123  9883
##  backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri = F)) 9883 10263
##      mean median      uq   max neval cld
##    661.49    760   760.5  3421   100 a  
##  10711.72  10263 10643.0 23947   100  b 
##  12467.75  11023 11404.0 61578   100   c</code></pre>
<p>Using the inverse once you have it is over 10 times faster, a significant speedup. I thought in the past this was more like 2, it probably depends on the size of the matrix.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>In summary, you should never invert a matrix. You either use <code>solve</code> to solve a single set of equations, or use a matrix decomposition for stability. However, it doesn’t seem like a huge difference numerically, and it can be a lot slower, so there are cases where you may just want to get the full inverse.</p>
</div>
