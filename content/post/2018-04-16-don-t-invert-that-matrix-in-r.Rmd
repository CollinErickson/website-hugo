---
title: Don't invert that matrix in R
author: Collin Erickson
date: '2018-04-16'
slug: don-t-invert-that-matrix-in-r
categories: []
tags: []
---

There's a somewhat famous blog post titled
[Don't Invert That Matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/).
This topic has also been discussed using R
[on this blog post](http://civilstat.com/2015/07/dont-invert-that-matrix-why-and-how/).
You should read those instead of this.
I'm just experimenting to see how important this concept is in R,
especially in the cases I use often.

## The main idea

Often finding the inverse of a matrix is not the endgoal.
If it is, then you have do invert it.
Otherwise you are usually trying to solve a system of equations, $Ax=b$,
for the vector $x$, where $A$ is a matrix and $b$ is a given vector.
You may need to solve this for many different $b$ or for a single one.
If you only need to solve it for a single $b$, you can simply run
`solve(A, b)` and you are done.

It becomes more of an issue when you need to solve the equation for many different $b$.
Using `solve` is an $O(n^3)$ operation, where $n$ is the number of rows of $A$,
meaning it is slow.
In small dimensions, you can just invert $A$ by storing the result of `solve(A)`,
and multiplying this by each $b$.
If you have all your $b$'s at once, say in matrix $B$, you can use
`solve(A, B)` and you are done.
So now the problem is really that you need to solve the equation for many different $b$,
and you don't have all the $b$'s in the beginning,
and that $A$ is big.
Think of having to solve an iterative equation.

Having to do an $O(n^3)$ operation repeatedly is slow and a waste of resources.
You could invert $A$ once, then multiply that inverse times each $b$ when needed.
The problem with this is that calculating the inverse is not as numerically stable
as factorizing the matrix.

## Factoring a matrix

Factoring a matrix means you find matrices whose product is equal to $A$.
An LU-decomposition (`Matrix::lu`) finds a lower triangular matrix $L$
and an upper triangular matrix $U$ such that $LU=A$.
If $A$ is not square, then it can be factored as $A=PLU$.
If $A$ as symmetric and positive definite, then the
Cholesky factorization (`chol`) gives an upper triangular matrix $R$
such that $R^T R = A$.
Other matrix decompositions include QR factorization and SVD.

Once you have a factorization of $A$, you can use the factors
for each $b$ to solve the system.
Usually the factors are triangular or diagonal.
The benefit of doing this is that it is much more numerically stable
to solve using the factors then with the full inverse of $A$.

## Testing with correlation matrices

I'm going to use correlation matrices to test this,
since that is what I use for Gaussian process models.
These matrices are positive definite,
so I can use the Cholesky decomposition.
Below I create a 100x100 correlation matrix and plot it.

```{r}
set.seed(0)
d <- 4
n <- 100
theta <- 2 #c(.05,.1,.05,1)
X <- matrix(runif(d*n), ncol=d)
A <- outer(1:n, 1:n,
           Vectorize(
             function(i,j) {
               exp(-sum(theta*(X[i,]-X[j,])^2))
             }
           ))
image(A)
```

To see how poorly conditioned it is, we can check the eigenvalues.

```{r}
summary(eigen(A, only.values = T)$values)
```
The smallest eigenvalue is about 0.00003, which is not great.
The determinant of the $A$ is `r det(A)`, which makes me think
it is poorly conditioned.

To see how the accuracy of the solve is,
I'll create a random vector $x$ and calculate $b$.


```{r}
set.seed(1)
x <- rnorm(100)
b <- A %*% x
```

First we'll check the difference between x and using `solve`.

```{r}
summary(solve(A, b) - x)
```
Largest difference is about 3e-11.

Now we can see how far off the solution is when using the inverse
compared to the correct answer.

```{r}
A_inv <- solve(A)
summary(A_inv %*% b - x)
```
The largest error is about 1.1e-10, which is 3x larger than when using `solve`.

Now using `backsolve` twice with the Cholesky factorization:
```{r}
A_chol <- chol(A)
summary(backsolve(A_chol, backsolve(A_chol, b, transpose = T)) - x)
```
Differences are as large as 2e-11, which is even smaller than
when using `solve`.

How about LU decomposition?
I tried first using `Matrix::lu`, but had issues.
Instead I used `matrixcalc::lu.decomposition`.

```{r}
library(matrixcalc)
A_lu <- lu.decomposition(A)
summary(c(A_lu$L %*% A_lu$U - A))
# Check that decomposition is correct, all < 1e-15
summary(backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri=F)) - x)
```
The biggest errors are about 3e-11, which is about the same as using `solve`,
better than using the inverse, and worse than using Cholesky.


So far it seems that doing the full inverse is the least accurate,
while the decompositions are about as accurate as using `solve`.

## Using Hilbert matrices

The symmetric Hilbert matrices are known for being numerically unstable.
I tried to invert the 20x20 Hilbert matrix, but R gave an error.
The largest that is invertible is the 11x11.

```{r}
set.seed(2)
n <- 11
A <- matrix(Matrix::Hilbert(n), n, n)
x <- rnorm(n)
b <- A %*% x
```

```{r}

A_inv <- solve(A)
summary(A_inv %*% b - x)
```


```{r}
A_chol <- chol(A)
summary(backsolve(A_chol, backsolve(A_chol, b, transpose = T)) - x)
```


```{r}
A_lu <- lu.decomposition(A)
summary(c(A_lu$L %*% A_lu$U - A))
# Check that decomposition is correct, all < 1e-15
summary(backsolve(A_lu$U, backsolve(A_lu$L, b, upper.tri=F)) - x)
```

