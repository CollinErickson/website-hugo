---
title: Creating an autoencoder with TensorFlow in R
author: Collin Erickson
date: '2018-08-22'
slug: creating-an-autoencoder-with-tensorflow-in-r
categories: []
tags: [R, TensorFlow, autoencoder]
---

```{r}
knitr::opts_chunk$set(cache=TRUE)
```

Once again I'm going to be trying something new,
and mainly just using this blog post to track it
for later reference.
This time I am going to implement an autoencoder,
and I'm going to use the R interface to TensorFlow
to do it.

## Data

I'm not going to use real data.
I'm just going to creating a function that will give a sequence
of numbers.
These will be fed into the network, with the goal being
to get the same thing out on the other side.

I'll just use values from a sine function.
Each training instance will have points generated
from a different set of parameters.
The parameters of a sine function are the phase,
frequency, and amplitude.

Here's what a single data instance will look like,
I'll evaluate the function at $0, \ldots, 100$.

```{r}
generate_instance <- function(x, phase, frequency, amplitude) {
  amplitude*sin(x*frequency-phase)
}
plot(generate_instance(0:100, 0, .1, 1.5))
```

Here's a function to get multiple instances, which will make
it easy to generate a batch for training.

```{r}
get_instances <- function(k) {
  sapply(1:k,
         function(xx) generate_instance(0:100, runif(1,0,100),runif(1, .03, .5),runif(1,.1,10))
         )
}
```

```{r plot3}

plot_3pairs <- function(x, y) {
  par(mfrow=c(1,3))
  for (i in 1:3) {
    plot(x[i,], cex=1.4)
    points(y[i,], col=2, pch=4)
  }
}
```


## Autoencoder

The key idea of an autoencoder is that if we expect the
output of a neural network to be equal to the input,
and if there is a layer in the network with fewer nodes than
the inputs,
then the values at this layer represent a sort of
data compression of the inputs.

So I'll randomly generate function parameters
(phase, frequency, and amplitude) to get a function,
evaluate this function at a set of inputs,
feed this into a neural network,
and have the loss function be minimized when
the outputs of the network are the same as the inputs.

## Loss function

The loss function can just be the MSE
(mean squared error).


## TensorFlow code

I haven't done much TensorFlow in R, I've done more
in Python, so I'm just learning as I go along here.
I'm going to look at some examples from 
[here](https://tensorflow.rstudio.com/learn/examples.html)
for guidance.

First I'll do a trivial network, where the 101 inputs
feed directly into the 101 outputs.
It should learn to put 1 on the weights connecting straight across,
and 0 everywhere else.

```{r}
library(tensorflow)

X <- tf$placeholder(tf$float32, list(NULL, 101))
#dense1 <- tf$layers$dense(X, 101, activation = tf$sigmoid)
Out <- tf$layers$dense(X, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

for (i in 1:2000) {
  batch <- t(get_instances(100))
  if (i %% 100 == 0) {
    train_mse <- mse$eval(feed_dict = dict(
      X = batch), session = sess)
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

We can see that the training MSE has decreased significantly.

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot(X1[1,], cex=1.4)
print(dim(preds))
points(preds[1,], col=2, pch=4)
```

From this example we can see that the model
has learned to output the same thing as the input.

Now I'll try the same thing, except I'll add a layer
between the inputs and outputs with fewer nodes.
Now the network will have to perform some sort of data
compression to get the outputs to match the inputs.
I'll also let it run for more batches since it
will be harder to learn the appropriate model weights.

The middle layer has 10 nodes and is densely connected
between the inputs and outputs.


```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 10, activation = tf$sigmoid)
Out <- tf$layers$dense(dense1, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

for (i in 1:10000) {
  batch <- t(get_instances(100))
  if (i %% 500 == 0) {
    train_mse <- mse$eval(feed_dict = dict(
      X = batch), session = sess)
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

The MSE hasn't gotten as small as before, but it has decreased a bit.
It definitely will do better with more training.
Let's look at some of the examples to see what these look like
with a fairly high MSE.

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot(X1[1,], cex=1.4)
points(preds[1,], col=2, pch=4)
plot(X1[2,], cex=1.4)
points(preds[2,], col=2, pch=4)
plot(X1[3,], cex=1.4)
points(preds[3,], col=2, pch=4)
```

This model was clearly undertrained, I'll try training it for longer.

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 10, activation = tf$sigmoid)
Out <- tf$layers$dense(dense1, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

for (i in 1:100000) {
  batch <- t(get_instances(100))
  if (i %% 5000 == 0) {
    train_mse <- mse$eval(feed_dict = dict(
      X = batch), session = sess)
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot(X1[1,], cex=1.4)
points(preds[1,], col=2, pch=4)
plot(X1[2,], cex=1.4)
points(preds[2,], col=2, pch=4)
plot(X1[3,], cex=1.4)
points(preds[3,], col=2, pch=4)
```

Add more layers again


```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense1 <- tf$layers$dense(X, 10, activation = tf$nn$selu)
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense1, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

for (i in 1:100000) {
  batch <- t(get_instances(100))
  if (i %% 5000 == 0) {
    train_mse <- mse$eval(feed_dict = dict(
      X = batch), session = sess)
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot(X1[1,], cex=1.4)
points(preds[1,], col=2, pch=4)
plot(X1[2,], cex=1.4)
points(preds[2,], col=2, pch=4)
plot(X1[3,], cex=1.4)
points(preds[3,], col=2, pch=4)
```

New plot func

```{r, fig.keep='last', fig.width=12, fig.height=4}
plot_3pairs(X1, preds)
```

