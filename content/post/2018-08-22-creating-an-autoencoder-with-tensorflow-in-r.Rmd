---
title: Creating an autoencoder with TensorFlow in R
author: Collin Erickson
date: '2018-08-22'
slug: creating-an-autoencoder-with-tensorflow-in-r
categories: []
tags: [R, TensorFlow, autoencoder]
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
set.seed(0)
```

Once again I'm going to be trying something new,
and mainly just using this blog post to track it
for later reference.
This time I am going to implement an autoencoder,
and I'm going to use the R interface to TensorFlow
to do it.

## Data

I'm not going to use real data.
I'm just going to creating a function that will give a sequence
of numbers.
These will be fed into the network, with the goal being
to get the same thing out on the other side.

I'll just use values from a sine function.
Each training instance will have points generated
from a different set of parameters.
The parameters of a sine function are the phase,
frequency, and amplitude.

Here's what a single data instance will look like,
I'll evaluate the function at $0, \ldots, 100$.

```{r}
generate_instance <- function(x, phase, frequency, amplitude) {
  amplitude*sin(x*frequency-phase)
}
plot(generate_instance(0:100, 0, .1, 1.5))
```

Here's a function to get multiple instances, which will make
it easy to generate a batch for training.

```{r}
get_instances <- function(k) {
  sapply(1:k,
         function(xx) generate_instance(0:100, runif(1,0,100),runif(1, .03, .5),runif(1,.1,10))
         )
}
```

Here is a function that will make it easy to plot three
input sets along with the model predictions.

```{r plot3}
plot_3pairs <- function(x, y) {
  par(mfrow=c(1,3))
  for (i in 1:3) {
    plot(x[i,], cex=1.4, ylab='')
    if (!missing(y)) points(y[i,], col=2, pch=4)
  }
}
```

Here are an example of what 3 randomly generated instances look like.

```{r, fig.keep='last', fig.width=12, fig.height=4}
plot_3pairs(t(get_instances(3)))
```

## Autoencoder

The key idea of an autoencoder is that if we expect the
output of a neural network to be equal to the input,
and if there is a layer in the network with fewer nodes than
the inputs,
then the values at this layer represent a sort of
data compression of the inputs.

So I'll randomly generate function parameters
(phase, frequency, and amplitude) to get a function,
evaluate this function at a set of inputs,
feed this into a neural network,
and have the loss function be minimized when
the outputs of the network are the same as the inputs.

## Loss function

The loss function can just be the MSE
(mean squared error).
This will guide the neural network to make as close an
approximation as possible over all the points.


## TensorFlow code

I haven't done much TensorFlow in R, I've done more
in Python, so I'm just learning as I go along here.
I'm going to look at some examples from 
[here](https://tensorflow.rstudio.com/learn/examples.html)
for guidance.

First I'll do a trivial network, where the 101 inputs
feed directly into the 101 outputs.
It should learn to put 1 on the weights connecting straight across,
and 0 everywhere else.

```{r, echo=FALSE}
set.seed(83274)
```


```{r}
library(tensorflow)

X <- tf$placeholder(tf$float32, list(NULL, 101))
#dense1 <- tf$layers$dense(X, 101, activation = tf$sigmoid)
Out <- tf$layers$dense(X, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 2000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 100 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

We can see that the training MSE has decreased significantly.

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
plot_3pairs(X1, preds)
```


From this example we can see that the model
has learned to output the same thing as the input.

## Adding another layer with 10 nodes in middle

Now I'll try the same thing, except I'll add a layer
between the inputs and outputs with fewer nodes.
Now the network will have to perform some sort of data
compression to get the outputs to match the inputs.
I'll also let it run for more batches since it
will be harder to learn the appropriate model weights.

The middle layer has 10 nodes and is densely connected
between the inputs and outputs.


```{r, echo=FALSE}
set.seed(81244)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 10, activation = tf$sigmoid)
Out <- tf$layers$dense(dense1, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 10000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 500 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
plot(mses, xlab="Batch number", ylab="MSE", type='l')
```

The MSE hasn't gotten as small as before, but it has decreased a bit.
It definitely will do better with more training.
Let's look at some of the examples to see what these look like
with a fairly high MSE.

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

This model was clearly undertrained, I'll try training it for longer.

```{r, echo=FALSE}
set.seed(191944)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 10, activation = tf$sigmoid)
Out <- tf$layers$dense(dense1, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 10000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
plot(mses, xlab="Batch number", ylab="MSE", type='l')
```

```{r}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot(X1[1,], cex=1.4)
points(preds[1,], col=2, pch=4)
plot(X1[2,], cex=1.4)
points(preds[2,], col=2, pch=4)
plot(X1[3,], cex=1.4)
points(preds[3,], col=2, pch=4)
```

Add more layers again


```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense2 <- tf$layers$dense(dense1, 10, activation = tf$nn$selu)
dense3 <- tf$layers$dense(dense2, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense3, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 100000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

We can see this network has learned to recreate the 101 inputs
with only 10 nodes in the middle layer.
This means that the information was compressed to be 1/10 of
its original size with nearly no loss.


## A middle layer with only 3 nodes

Since the data is created using three parameters
(frequency, amplitude, and phase),
we should be able to recreated the inputs even if there
is a middle layer with as few as 3 nodes.

This will likely take longer to train, and maybe need more
intermediate layers to calculate the required information.
First I'll just run it with the same 101-50-3-50-101 structure
to see how it does.

```{r, echo=FALSE}
set.seed(4912)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense2 <- tf$layers$dense(dense1, 3, activation = tf$nn$selu)
dense3 <- tf$layers$dense(dense2, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense3, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 100000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

And 3 more:

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

This is really good. There are a few places where the red and
black don't exactly align, but it's very close.
I'm surprised it didn't need more layers.


## Adding more layers, still 3 nodes in middle

The MSE had leveled out but the accuracies weren't
that great in the last example.
It may be that it needs more layers in order to compress
the function parameters into the three nodes of
the middle layer.

```{r, echo=FALSE}
set.seed(55182)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense1b <- tf$layers$dense(dense1, 20, activation = tf$nn$selu)
dense2 <- tf$layers$dense(dense1b, 3, activation = tf$nn$selu)
dense2b <- tf$layers$dense(dense2, 20, activation = tf$nn$selu)
dense3 <- tf$layers$dense(dense2b, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense3, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 100000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

And 3 more:

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```








## Two nodes in the middle layer

With only two nodes in the middle layer,
the network won't be able to keep information
for three independent parameters.
Let's see how it does with the same network, with
the only difference being reducing the middle layer
nodes from three to two.

```{r, echo=FALSE}
set.seed(3328)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense2 <- tf$layers$dense(dense1, 2, activation = tf$nn$selu)
dense3 <- tf$layers$dense(dense2, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense3, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 100000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```

This is still almost perfect,
but it shouldn't be able to do this.
I'm going to plot another set of inputs and outputs.

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```


## One node

```{r, echo=FALSE}
set.seed(7732)
```

```{r}
X <- tf$placeholder(tf$float32, list(NULL, 101))
dense1 <- tf$layers$dense(X, 50, activation = tf$nn$selu)
dense2 <- tf$layers$dense(dense1, 1, activation = tf$nn$selu)
dense3 <- tf$layers$dense(dense2, 50, activation = tf$nn$selu)
Out <- tf$layers$dense(dense3, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

N <- 100000
mses <- numeric(N)
for (i in 1:N) {
  batch <- t(get_instances(100))
  train_mse <- mse$eval(feed_dict = dict(
    X = batch), session = sess)
  mses[i] <- train_mse
  if (i %% 5000 == 0) {
    cat(sprintf("step %d, training MSE %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```

```{r}
plot(mses, xlab="Batch number", ylab="MSE (log scale)", type='l', log='y')
```

```{r, fig.keep='last', fig.width=12, fig.height=4}
X1 <- t(get_instances(3))
preds <- sess$run(Out, feed_dict=dict(X=X1))
plot_3pairs(X1, preds)
```
