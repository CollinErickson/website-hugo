---
title: Creating an autoencoder with TensorFlow in R
author: Collin Erickson
date: '2018-08-22'
slug: creating-an-autoencoder-with-tensorflow-in-r
categories: []
tags: [R, TensorFlow, autoencoder]
---

Once again I'm going to be trying something new,
and mainly just using this blog post to track it
for later reference.
This time I am going to implement an autoencoder,
and I'm going to use the R interface to TensorFlow
to do it.

## Data

I'm not going to use real data.
I'm just going to creating a function that will give a sequence
of numbers.
These will be fed into the network, with the goal being
to get the same thing out on the other side.

I'll just use values from a sine function.
Each training instance will have points generated
from a different set of parameters.
The parameters of a sine function are the phase,
frequency, and amplitude.

Here's what a single data instance will look like,
I'll evaluate the function at $0, \ldots, 100$.

```{r}
generate_instance <- function(x, phase, frequency, amplitude) {
  amplitude*sin(x*frequency-phase)
}
plot(generate_instance(0:100, 0, .1, 1.5))
```

Here's a function to get multiple instances, which will make
it easy to generate a batch for training.

```{r}
get_instances <- function(k) {
  sapply(1:k,
         function(xx) generate_instance(0:100, runif(1,0,100),runif(1, .03, .5),runif(1,.1,10))
         )
}
```


## Autoencoder

The key idea of an autoencoder is that if we expect the
output of a neural network to be equal to the input,
and if there is a layer in the network with fewer nodes than
the inputs,
then the values at this layer represent a sort of
data compression of the inputs.

So I'll randomly generate function parameters
(phase, frequency, and amplitude) to get a function,
evaluate this function at a set of inputs,
feed this into a neural network,
and have the loss function be minimized when
the outputs of the network are the same as the inputs.

## Loss function

The loss function can just be the MSE
(mean squared error).


## TensorFlow code

I haven't done much TensorFlow in R, I've done more
in Python, so I'm just learning as I go along here.
I'm going to look at some examples from 
[here](https://tensorflow.rstudio.com/learn/examples.html)
for guidance.

```{r}
library(tensorflow)

X <- tf$placeholder(tf$float32, list(NULL, 101))
#dense1 <- tf$layers$dense(X, 101, activation = tf$sigmoid)
Out <- tf$layers$dense(X, 101) # Default activation is linear

mse <- tf$reduce_mean(tf$square(X - Out))
train_step <- tf$train$AdamOptimizer(1e-3)$minimize(mse)

sess <-  tf$Session()
sess$run(tf$global_variables_initializer())

for (i in 1:2000) {
  batch <- t(get_instances(100))
  if (i %% 100 == 0) {
    #train_accuracy <- accuracy$eval(feed_dict = dict(
    #    x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))
    train_mse <- mse$eval(feed_dict = dict(
      X = batch), session = sess)
    cat(sprintf("step %d, training accuracy %g\n", i, train_mse))
  }
  train_step$run(feed_dict = dict(X = batch), session = sess)
}
```